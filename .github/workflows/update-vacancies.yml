name: Update Vacancies Fixed

on:
  schedule:
    - cron: '0 * * * *' # –ó–∞–ø—É—Å–∫ –∫–∞–∂–¥—ã–π —á–∞—Å
  workflow_dispatch: # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫

jobs:
  update-vacancies:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
          
      - name: Create enhanced vacancy collector script
        run: |
          cat > collect_vacancies.py << 'EOF'
          import requests
          import json
          import time
          from datetime import datetime, timedelta
          from typing import List, Dict, Optional, Set
          import re
          import os
          
          # API HH.ru
          BASE_URL = "https://api.hh.ru/vacancies"
          
          # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
          HEADERS = {
              'User-Agent': 'VacancyAggregator/2.1 (https://gradelift.ru)'
          }
          
          # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
          REQUEST_DELAY = 0.3
          
          # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
          SEARCH_KEYWORDS = [
              '—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä',
              '—Å–∏—Å–∞–¥–º–∏–Ω',
              'system administrator'
          ]
          
          
          def get_vacancies_with_pagination_fix(keyword: str, region_id: str = '1') -> tuple[List[Dict], Dict]:
              """–ü–æ–ª—É—á–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –æ–±—Ö–æ–¥–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
              print(f"\n{'='*60}")
              print(f"üîç –ü–æ–∏—Å–∫: '{keyword}' –≤ –ú–æ—Å–∫–≤–µ")
              print(f"{'='*60}")
              
              all_vacancies = []
              stats = {
                  'found': 0,
                  'collected': 0,
                  'method': 'standard'
              }
              
              # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—ä–µ–º–∞
              params = {
                  'text': keyword,
                  'area': region_id,
                  'search_field': 'name',
                  'per_page': '50',  # –ö–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ HH
                  'page': '0'
              }
              
              try:
                  response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)
                  if response.status_code == 200:
                      data = response.json()
                      stats['found'] = data.get('found', 0)
                      total_pages = data.get('pages', 0)
                      
                      print(f"–ù–∞–π–¥–µ–Ω–æ: {stats['found']} –≤–∞–∫–∞–Ω—Å–∏–π")
                      print(f"–°—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
                      
                      # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 20 —Å—Ç—Ä–∞–Ω–∏—Ü - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
                      if total_pages > 20:
                          print(f"‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü > 20")
                          stats['method'] = 'segmented'
                          all_vacancies = get_with_date_segmentation(keyword, region_id)
                      else:
                          # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                          all_vacancies = collect_all_pages(params, total_pages)
                      
                      stats['collected'] = len(all_vacancies)
                  
              except Exception as e:
                  print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
              
              print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ: {stats['collected']} –∏–∑ {stats['found']} ({stats['method']})")
              return all_vacancies, stats
          
          
          def collect_all_pages(base_params: Dict, total_pages: int) -> List[Dict]:
              """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
              all_items = []
              
              for page in range(min(total_pages, 20)):  # –ú–∞–∫—Å–∏–º—É–º 20 —Å—Ç—Ä–∞–Ω–∏—Ü
                  params = base_params.copy()
                  params['page'] = str(page)
                  
                  try:
                      response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)
                      if response.status_code == 200:
                          data = response.json()
                          items = data.get('items', [])
                          
                          if not items:
                              print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø—É—Å—Ç–∞—è")
                              break
                          
                          all_items.extend(items)
                          print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {len(items)} –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: {len(all_items)})")
                          
                          time.sleep(REQUEST_DELAY)
                      else:
                          print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –æ—à–∏–±–∫–∞ {response.status_code}")
                          break
                          
                  except Exception as e:
                      print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –æ—à–∏–±–∫–∞ {e}")
                      break
              
              return all_items
          
          
          def get_with_date_segmentation(keyword: str, region_id: str) -> List[Dict]:
              """–ü–æ–ª—É—á–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–∞–º"""
              print("\nüìÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –¥–∞—Ç–∞–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
              
              all_vacancies = []
              unique_ids = set()
              
              # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
              segments = [
                  {'days': 1, 'name': '–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞'},
                  {'days': 3, 'name': '–ó–∞ 2-3 –¥–Ω—è'},
                  {'days': 7, 'name': '–ó–∞ 4-7 –¥–Ω–µ–π'},
                  {'days': 14, 'name': '–ó–∞ 8-14 –¥–Ω–µ–π'},
                  {'days': 30, 'name': '–ó–∞ 15-30 –¥–Ω–µ–π'}
              ]
              
              end_date = datetime.now()
              
              for i, segment in enumerate(segments):
                  if i == 0:
                      date_from = end_date - timedelta(days=segment['days'])
                      date_to = end_date
                  else:
                      date_from = end_date - timedelta(days=segment['days'])
                      date_to = end_date - timedelta(days=segments[i-1]['days'])
                  
                  print(f"\nüîç {segment['name']}")
                  print(f"   –ü–µ—Ä–∏–æ–¥: {date_from.strftime('%Y-%m-%d')} - {date_to.strftime('%Y-%m-%d')}")
                  
                  params = {
                      'text': keyword,
                      'area': region_id,
                      'search_field': 'name',
                      'date_from': date_from.strftime('%Y-%m-%d'),
                      'date_to': date_to.strftime('%Y-%m-%d'),
                      'per_page': '50',
                      'page': '0'
                  }
                  
                  # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
                  segment_vacancies = []
                  page = 0
                  
                  while True:
                      params['page'] = str(page)
                      
                      try:
                          response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)
                          if response.status_code == 200:
                              data = response.json()
                              
                              if page == 0:
                                  found_in_segment = data.get('found', 0)
                                  print(f"   –ù–∞–π–¥–µ–Ω–æ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ: {found_in_segment}")
                              
                              items = data.get('items', [])
                              if not items:
                                  break
                              
                              segment_vacancies.extend(items)
                              
                              if page >= data.get('pages', 0) - 1:
                                  break
                              
                              if page >= 19:  # –ú–∞–∫—Å–∏–º—É–º 20 —Å—Ç—Ä–∞–Ω–∏—Ü
                                  print(f"   –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü")
                                  break
                              
                              page += 1
                              time.sleep(REQUEST_DELAY)
                          else:
                              break
                      except Exception as e:
                          print(f"   –û—à–∏–±–∫–∞: {e}")
                          break
                  
                  # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
                  new_count = 0
                  for item in segment_vacancies:
                      vacancy_id = item.get('id')
                      if vacancy_id and vacancy_id not in unique_ids:
                          unique_ids.add(vacancy_id)
                          all_vacancies.append(item)
                          new_count += 1
                  
                  print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {new_count}")
                  print(f"   –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_vacancies)}")
              
              return all_vacancies
          
          
          def clean_html(html_text: str) -> str:
              """–û—á–∏—â–∞–µ—Ç HTML —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
              if not html_text:
                  return ""
              clean = re.sub('<.*?>', '', html_text)
              clean = clean.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
              clean = ' '.join(clean.split())
              return clean
          
          
          def format_salary(salary_data: Dict) -> str:
              """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ"""
              if not salary_data:
                  return "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"
              
              try:
                  salary_from = salary_data.get('from')
                  salary_to = salary_data.get('to')
                  currency = salary_data.get('currency', 'RUR')
                  gross = salary_data.get('gross', False)
                  
                  if salary_from:
                      salary_from = f"{salary_from:,}".replace(',', ' ')
                  if salary_to:
                      salary_to = f"{salary_to:,}".replace(',', ' ')
                  
                  if salary_from and salary_to:
                      result = f"{salary_from} - {salary_to} {currency}"
                  elif salary_from:
                      result = f"–æ—Ç {salary_from} {currency}"
                  elif salary_to:
                      result = f"–¥–æ {salary_to} {currency}"
                  else:
                      return "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                  
                  if gross:
                      result += " –¥–æ –≤—ã—á–µ—Ç–∞ –Ω–∞–ª–æ–≥–æ–≤"
                  else:
                      result += " –Ω–∞ —Ä—É–∫–∏"
                  
                  return result
              except Exception:
                  return "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"
          
          
          def safe_get(data: Optional[Dict], *keys) -> any:
              """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
              if data is None:
                  return None
              result = data
              for key in keys:
                  if isinstance(result, dict):
                      result = result.get(key)
                      if result is None:
                          return None
                  else:
                      return None
              return result
          
          
          def parse_vacancy(item: Dict) -> Dict:
              """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
              try:
                  employer = item.get('employer') or {}
                  logo_urls = employer.get('logo_urls') or {}
                  company_logo = logo_urls.get('original', '')
                  
                  vacancy = {
                      'id': item.get('id', ''),
                      'name': item.get('name', ''),
                      'company': employer.get('name', ''),
                      'company_id': employer.get('id', ''),
                      'company_url': employer.get('alternate_url', ''),
                      'company_logo': company_logo,
                      'url': item.get('alternate_url', ''),
                      'published_at': item.get('published_at', ''),
                      'created_at': item.get('created_at', ''),
                      'area': safe_get(item, 'area', 'name') or '',
                      'salary': format_salary(item.get('salary')),
                      'salary_raw': item.get('salary'),
                      'experience': safe_get(item, 'experience', 'name') or '',
                      'schedule': safe_get(item, 'schedule', 'name') or '',
                      'employment': safe_get(item, 'employment', 'name') or '',
                      'requirement': clean_html(safe_get(item, 'snippet', 'requirement') or ''),
                      'responsibility': clean_html(safe_get(item, 'snippet', 'responsibility') or ''),
                      'type': safe_get(item, 'type', 'name') or '',
                      'professional_roles': [],
                      'has_test': item.get('has_test', False),
                      'premium': item.get('premium', False),
                      'accept_handicapped': item.get('accept_handicapped', False),
                      'accept_kids': item.get('accept_kids', False),
                      'accept_temporary': item.get('accept_temporary', False)
                  }
                  
                  roles = item.get('professional_roles', [])
                  if isinstance(roles, list):
                      vacancy['professional_roles'] = [role.get('name', '') for role in roles if isinstance(role, dict)]
                  
                  return vacancy
                  
              except Exception as e:
                  print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
                  return {
                      'id': item.get('id', ''),
                      'name': item.get('name', '–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏'),
                      'error': str(e)
                  }
          
          
          def collect_all_vacancies() -> List[Dict]:
              """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
              print("=== –°–ë–û–† –í–ê–ö–ê–ù–°–ò–ô –°–ò–°–¢–ï–ú–ù–´–• –ê–î–ú–ò–ù–ò–°–¢–†–ê–¢–û–†–û–í –í –ú–û–°–ö–í–ï ===")
              print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now()}")
              print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(SEARCH_KEYWORDS)}")
              print("–ú–µ—Ç–æ–¥: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π (—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)")
              
              unique_vacancy_ids: Set[str] = set()
              all_vacancies: List[Dict] = []
              
              total_stats = {
                  'total_found': 0,
                  'total_collected': 0,
                  'by_keyword': {},
                  'duplicates': 0
              }
              
              for keyword in SEARCH_KEYWORDS:
                  vacancies, stats = get_vacancies_with_pagination_fix(keyword, '1')
                  
                  total_stats['by_keyword'][keyword] = stats
                  total_stats['total_found'] += stats['found']
                  
                  new_count = 0
                  for item in vacancies:
                      try:
                          vacancy_id = item.get('id')
                          if vacancy_id and vacancy_id not in unique_vacancy_ids:
                              unique_vacancy_ids.add(vacancy_id)
                              vacancy = parse_vacancy(item)
                              all_vacancies.append(vacancy)
                              new_count += 1
                          else:
                              total_stats['duplicates'] += 1
                              
                      except Exception as e:
                          print(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏: {e}")
                          continue
                  
                  print(f"üìå –î–æ–±–∞–≤–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {new_count}")
                  print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(all_vacancies)}")
              
              total_stats['total_collected'] = len(all_vacancies)
              
              # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
              print(f"\n{'='*60}")
              print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
              print(f"{'='*60}")
              
              for keyword, stats in total_stats['by_keyword'].items():
                  completeness = (stats['collected'] / stats['found'] * 100) if stats['found'] > 0 else 0
                  print(f"'{keyword}':")
                  print(f"  - –ù–∞–π–¥–µ–Ω–æ: {stats['found']}")
                  print(f"  - –°–æ–±—Ä–∞–Ω–æ: {stats['collected']} ({completeness:.1f}%)")
                  print(f"  - –ú–µ—Ç–æ–¥: {stats['method']}")
              
              print(f"\n–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {total_stats['duplicates']}")
              print(f"–í–°–ï–ì–û —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {total_stats['total_collected']}")
              
              # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
              try:
                  all_vacancies.sort(key=lambda x: x.get('published_at', ''), reverse=True)
              except Exception as e:
                  print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
              
              return all_vacancies
          
          
          def save_vacancies(vacancies: List[Dict], filename: str = 'hh_vacancies.json'):
              """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ JSON —Ñ–∞–π–ª"""
              stats = {
                  'total': len(vacancies),
                  'with_salary': sum(1 for v in vacancies if v.get('salary', '–Ω–µ —É–∫–∞–∑–∞–Ω–∞') != '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'),
                  'companies': len(set(v.get('company', '') for v in vacancies if v.get('company'))),
                  'cities': len(set(v.get('area', '') for v in vacancies if v.get('area'))),
                  'premium': sum(1 for v in vacancies if v.get('premium', False)),
                  'with_test': sum(1 for v in vacancies if v.get('has_test', False))
              }
              
              # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä–∞—Ñ–∏–∫–∞–º —Ä–∞–±–æ—Ç—ã
              schedules = {}
              for v in vacancies:
                  schedule = v.get('schedule', '–ù–µ —É–∫–∞–∑–∞–Ω')
                  schedules[schedule] = schedules.get(schedule, 0) + 1
              
              # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º —Å–±–æ—Ä–∞
              methods_used = []
              if any('segmented' in str(v) for v in vacancies):
                  methods_used.append('–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –¥–∞—Ç–∞–º')
              else:
                  methods_used.append('–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è')
              
              output = {
                  'source': 'hh.ru',
                  'search_keywords': SEARCH_KEYWORDS,
                  'search_params': {
                      'area': '–ú–æ—Å–∫–≤–∞',
                      'area_id': '1',
                      'search_field': '–í –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏',
                      'filter': '–ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–æ–≤',
                      'method': '–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π',
                      'per_page': '50 (–∫–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ HH)'
                  },
                  'updated': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                  'statistics': stats,
                  'schedule_distribution': schedules,
                  'vacancies': vacancies
              }
              
              # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª
              if os.path.exists(filename):
                  os.remove(filename)
                  print(f"   üóëÔ∏è –°—Ç–∞—Ä—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
              
              with open(filename, 'w', encoding='utf-8') as f:
                  json.dump(output, f, ensure_ascii=False, indent=2)
              
              print(f"\n‚úÖ –§–∞–π–ª {filename} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
              print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
              for key, value in stats.items():
                  print(f"   - {key}: {value}")
              
              print(f"\nüìÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥—Ä–∞—Ñ–∏–∫—É —Ä–∞–±–æ—Ç—ã:")
              for schedule, count in sorted(schedules.items(), key=lambda x: x[1], reverse=True)[:5]:
                  percent = (count / len(vacancies) * 100) if vacancies else 0
                  print(f"   - {schedule}: {count} ({percent:.1f}%)")
          
          
          def main():
              """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
              try:
                  vacancies = collect_all_vacancies()
                  
                  if not vacancies:
                      print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏")
                      empty_output = {
                          'source': 'hh.ru',
                          'search_keywords': SEARCH_KEYWORDS,
                          'search_params': {
                              'area': '–ú–æ—Å–∫–≤–∞',
                              'area_id': '1',
                              'search_field': '–í –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏',
                              'filter': '–ë–ï–ó —Ñ–∏–ª—å—Ç—Ä–æ–≤'
                          },
                          'updated': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                          'statistics': {
                              'total': 0,
                              'with_salary': 0,
                              'companies': 0,
                              'cities': 0,
                              'premium': 0,
                              'with_test': 0
                          },
                          'schedule_distribution': {},
                          'vacancies': []
                      }
                      with open('hh_vacancies.json', 'w', encoding='utf-8') as f:
                          json.dump(empty_output, f, ensure_ascii=False, indent=2)
                      return False
                  
                  save_vacancies(vacancies)
                  
                  # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π
                  try:
                      companies = {}
                      for v in vacancies:
                          company = v.get('company', '')
                          if company:
                              companies[company] = companies.get(company, 0) + 1
                      
                      if companies:
                          top_companies = sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]
                          print("\nüè¢ –¢–æ–ø-5 –∫–æ–º–ø–∞–Ω–∏–π:")
                          for company, count in top_companies:
                              print(f"   - {company}: {count} –≤–∞–∫–∞–Ω—Å–∏–π")
                  except Exception as e:
                      print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Ç–æ–ø –∫–æ–º–ø–∞–Ω–∏–π: {e}")
                  
                  print("\n‚ú® –ì–æ—Ç–æ–≤–æ!")
                  return True
                  
              except Exception as e:
                  print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
                  import traceback
                  traceback.print_exc()
                  
                  error_output = {
                      'source': 'hh.ru',
                      'error': str(e),
                      'updated': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                      'vacancies': []
                  }
                  with open('hh_vacancies.json', 'w', encoding='utf-8') as f:
                      json.dump(error_output, f, ensure_ascii=False, indent=2)
                  
                  return False
          
          
          if __name__ == "__main__":
              success = main()
              exit(0 if success else 1)
          EOF
          
      - name: Collect vacancies
        run: |
          echo "=== –°—Ç–∞—Ä—Ç —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π ==="
          python collect_vacancies.py || echo "–°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º"
          
      - name: Verify results
        run: |
          echo ""
          echo "=== –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ==="
          if [ -f "hh_vacancies.json" ]; then
            FILE_SIZE=$(stat -f%z "hh_vacancies.json" 2>/dev/null || stat -c%s "hh_vacancies.json")
            VACANCY_COUNT=$(grep -o '"id"' hh_vacancies.json | wc -l || echo "0")
            echo "‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω"
            echo "üìÅ –†–∞–∑–º–µ—Ä: $FILE_SIZE –±–∞–π—Ç"
            echo "üìä –í–∞–∫–∞–Ω—Å–∏–π: $VACANCY_COUNT"
            
            if [ "$VACANCY_COUNT" -gt 0 ]; then
              echo ""
              echo "üîç –ü—Ä–∏–º–µ—Ä—ã –≤–∞–∫–∞–Ω—Å–∏–π:"
              grep -A1 '"name"' hh_vacancies.json | head -10 || echo "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã"
              
              echo ""
              echo "üìÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä–∞—Ñ–∏–∫–∞–º —Ä–∞–±–æ—Ç—ã:"
              grep -o '"schedule":[^,]*' hh_vacancies.json | sort | uniq -c | sort -nr | head -10 || echo "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"
            fi
          else
            echo "‚ùå –§–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω!"
            echo '{"source":"hh.ru","vacancies":[],"updated":"'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"}' > hh_vacancies.json
          fi
          
      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add hh_vacancies.json
          
          if git diff --staged --quiet; then
            echo "‚ö†Ô∏è –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –∫–æ–º–º–∏—Ç–∞"
          else
            VACANCY_COUNT=$(grep -o '"id"' hh_vacancies.json | wc -l || echo "0")
            git commit -m "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è): $VACANCY_COUNT –≤–∞–∫–∞–Ω—Å–∏–π [$(date '+%Y-%m-%d %H:%M')]"
            git push
          fi
          
      - name: Deploy to FTP
        if: always()
        uses: SamKirkland/FTP-Deploy-Action@v4.3.5
        with:
          server: ${{ secrets.FTP_SERVER }}
          username: ${{ secrets.FTP_USERNAME }}
          password: ${{ secrets.FTP_PASSWORD }}
          protocol: ftp
          port: 21
          local-dir: "./"
          server-dir: "www/Vacancy/"
          state-name: ".ftp-deploy-sync-state.json"
          dry-run: false
          log-level: standard
          exclude: |
            **/.git*
            **/.git*/**
            **/node_modules/**
            **/.github/**
            **/README.md
            **/*.py
            **/.ftp-deploy-sync-state.json
