name: Update Vacancies

on:
  schedule:
    # –ó–∞–ø—É—Å–∫ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
    - cron: '0 */6 * * *'
  workflow_dispatch: # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫

jobs:
  update-vacancies:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å–±–æ—Ä–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
          
      - name: Create enhanced vacancy collector script
        run: |
          cat > collect_vacancies.py << 'EOF'
          import requests
          import json
          import time
          from datetime import datetime
          from typing import List, Dict, Optional, Set
          import re
          
          # API HH.ru
          BASE_URL = "https://api.hh.ru/vacancies"
          VACANCY_DETAIL_URL = "https://api.hh.ru/vacancies/{id}"
          
          # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
          HEADERS = {
              'User-Agent': 'VacancyAggregator/2.0 (https://gradelift.ru)'
          }
          
          # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
          REQUEST_DELAY = 0.3
          
          # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
          SEARCH_KEYWORDS = [
              '—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä',
              '—Å–∏—Å–∞–¥–º–∏–Ω',
              'system administrator'
          ]
          
          
          def get_vacancies_by_keyword(keyword: str) -> List[Dict]:
              """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –æ–¥–Ω–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É"""
              print(f"\nüîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}'")
              
              params = {
                  'text': keyword,
                  'area': '113',              # –†–æ—Å—Å–∏—è
                  'schedule': 'remote',       # –£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞
                  'search_field': 'name',     # –ò—Å–∫–∞—Ç—å –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
                  'per_page': '100',          # –ú–∞–∫—Å–∏–º—É–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                  'page': '0'
              }
              
              all_vacancies = []
              page = 0
              total_pages = None
              
              while True:
                  params['page'] = str(page)
                  
                  try:
                      response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)
                      
                      if response.status_code == 200:
                          data = response.json()
                          
                          if total_pages is None:
                              total_pages = data.get('pages', 0)
                              total_found = data.get('found', 0)
                              print(f"   –ù–∞–π–¥–µ–Ω–æ: {total_found} –≤–∞–∫–∞–Ω—Å–∏–π ({total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
                          
                          items = data.get('items', [])
                          all_vacancies.extend(items)
                          
                          page += 1
                          if page >= total_pages:
                              break
                          
                          time.sleep(REQUEST_DELAY)
                      else:
                          print(f"   ‚ùå –û—à–∏–±–∫–∞ API: {response.status_code}")
                          break
                          
                  except Exception as e:
                      print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                      break
              
              print(f"   ‚úÖ –°–æ–±—Ä–∞–Ω–æ: {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
              return all_vacancies
          
          
          def clean_html(html_text: str) -> str:
              """–û—á–∏—â–∞–µ—Ç HTML —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
              if not html_text:
                  return ""
              clean = re.sub('<.*?>', '', html_text)
              clean = clean.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
              clean = ' '.join(clean.split())
              return clean
          
          
          def format_salary(salary_data: Dict) -> str:
              """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ"""
              if not salary_data:
                  return "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"
              
              salary_from = salary_data.get('from')
              salary_to = salary_data.get('to')
              currency = salary_data.get('currency', 'RUR')
              gross = salary_data.get('gross', False)
              
              if salary_from:
                  salary_from = f"{salary_from:,}".replace(',', ' ')
              if salary_to:
                  salary_to = f"{salary_to:,}".replace(',', ' ')
              
              if salary_from and salary_to:
                  result = f"{salary_from} - {salary_to} {currency}"
              elif salary_from:
                  result = f"–æ—Ç {salary_from} {currency}"
              elif salary_to:
                  result = f"–¥–æ {salary_to} {currency}"
              else:
                  return "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"
              
              if gross:
                  result += " –¥–æ –≤—ã—á–µ—Ç–∞ –Ω–∞–ª–æ–≥–æ–≤"
              else:
                  result += " –Ω–∞ —Ä—É–∫–∏"
              
              return result
          
          
          def parse_vacancy(item: Dict) -> Dict:
              """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏"""
              vacancy = {
                  'id': item.get('id', ''),
                  'name': item.get('name', ''),
                  'company': item.get('employer', {}).get('name', ''),
                  'company_id': item.get('employer', {}).get('id', ''),
                  'company_url': item.get('employer', {}).get('alternate_url', ''),
                  'company_logo': item.get('employer', {}).get('logo_urls', {}).get('original', ''),
                  'url': item.get('alternate_url', ''),
                  'published_at': item.get('published_at', ''),
                  'created_at': item.get('created_at', ''),
                  'area': item.get('area', {}).get('name', ''),
                  'salary': format_salary(item.get('salary')),
                  'salary_raw': item.get('salary'),
                  'experience': item.get('experience', {}).get('name', ''),
                  'schedule': item.get('schedule', {}).get('name', ''),
                  'employment': item.get('employment', {}).get('name', ''),
                  'requirement': clean_html(item.get('snippet', {}).get('requirement', '')),
                  'responsibility': clean_html(item.get('snippet', {}).get('responsibility', '')),
                  'type': item.get('type', {}).get('name', ''),
                  'professional_roles': [role.get('name', '') for role in item.get('professional_roles', [])],
                  'has_test': item.get('has_test', False),
                  'premium': item.get('premium', False),
                  'accept_handicapped': item.get('accept_handicapped', False),
                  'accept_kids': item.get('accept_kids', False),
                  'accept_temporary': item.get('accept_temporary', False)
              }
              
              return vacancy
          
          
          def collect_all_vacancies() -> List[Dict]:
              """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
              print("=== –°–ë–û–† –í–°–ï–• –í–ê–ö–ê–ù–°–ò–ô –ü–û –°–ò–°–¢–ï–ú–ù–û–ú–£ –ê–î–ú–ò–ù–ò–°–¢–†–ò–†–û–í–ê–ù–ò–Æ ===")
              print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now()}")
              print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞: {', '.join(SEARCH_KEYWORDS)}")
              
              unique_vacancy_ids: Set[str] = set()
              all_vacancies: List[Dict] = []
              
              for keyword in SEARCH_KEYWORDS:
                  vacancies = get_vacancies_by_keyword(keyword)
                  
                  new_count = 0
                  for item in vacancies:
                      vacancy_id = item.get('id')
                      if vacancy_id and vacancy_id not in unique_vacancy_ids:
                          unique_vacancy_ids.add(vacancy_id)
                          vacancy = parse_vacancy(item)
                          all_vacancies.append(vacancy)
                          new_count += 1
                          
                          if len(all_vacancies) % 25 == 0:
                              print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(all_vacancies)}")
                  
                  print(f"   üìå –ù–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {new_count}")
              
              all_vacancies.sort(key=lambda x: x.get('published_at', ''), reverse=True)
              
              return all_vacancies
          
          
          def save_vacancies(vacancies: List[Dict], filename: str = 'hh_vacancies.json'):
              """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ JSON —Ñ–∞–π–ª"""
              stats = {
                  'total': len(vacancies),
                  'with_salary': sum(1 for v in vacancies if v['salary'] != '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'),
                  'companies': len(set(v['company'] for v in vacancies if v['company'])),
                  'cities': len(set(v['area'] for v in vacancies if v['area'])),
                  'premium': sum(1 for v in vacancies if v.get('premium', False)),
                  'with_test': sum(1 for v in vacancies if v.get('has_test', False))
              }
              
              output = {
                  'source': 'hh.ru',
                  'search_keywords': SEARCH_KEYWORDS,
                  'search_params': {
                      'area': '–†–æ—Å—Å–∏—è',
                      'schedule': '–£–¥–∞–ª—ë–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞',
                      'search_field': '–í –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏'
                  },
                  'updated': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                  'statistics': stats,
                  'vacancies': vacancies
              }
              
              with open(filename, 'w', encoding='utf-8') as f:
                  json.dump(output, f, ensure_ascii=False, indent=2)
              
              print(f"\n‚úÖ –§–∞–π–ª {filename} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
              print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
              for key, value in stats.items():
                  print(f"   - {key}: {value}")
          
          
          def main():
              """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
              try:
                  vacancies = collect_all_vacancies()
                  
                  if not vacancies:
                      print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏")
                      return False
                  
                  save_vacancies(vacancies)
                  
                  # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π
                  companies = {}
                  for v in vacancies:
                      company = v['company']
                      if company:
                          companies[company] = companies.get(company, 0) + 1
                  
                  top_companies = sorted(companies.items(), key=lambda x: x[1], reverse=True)[:5]
                  print("\nüè¢ –¢–æ–ø-5 –∫–æ–º–ø–∞–Ω–∏–π:")
                  for company, count in top_companies:
                      print(f"   - {company}: {count} –≤–∞–∫–∞–Ω—Å–∏–π")
                  
                  print("\n‚ú® –ì–æ—Ç–æ–≤–æ!")
                  return True
                  
              except Exception as e:
                  print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
                  import traceback
                  traceback.print_exc()
                  return False
          
          
          if __name__ == "__main__":
              success = main()
              exit(0 if success else 1)
          EOF
          
      - name: Collect vacancies
        run: |
          echo "=== –°—Ç–∞—Ä—Ç —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π ==="
          python collect_vacancies.py
          
      - name: Verify results
        run: |
          echo ""
          echo "=== –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ==="
          if [ -f "hh_vacancies.json" ]; then
            FILE_SIZE=$(stat -f%z "hh_vacancies.json" 2>/dev/null || stat -c%s "hh_vacancies.json")
            VACANCY_COUNT=$(grep -o '"id"' hh_vacancies.json | wc -l)
            echo "‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω"
            echo "üìÅ –†–∞–∑–º–µ—Ä: $FILE_SIZE –±–∞–π—Ç"
            echo "üìä –í–∞–∫–∞–Ω—Å–∏–π: $VACANCY_COUNT"
            echo ""
            echo "üîç –ü—Ä–∏–º–µ—Ä—ã –≤–∞–∫–∞–Ω—Å–∏–π:"
            grep -A1 '"name"' hh_vacancies.json | head -10
          else
            echo "‚ùå –§–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω!"
            exit 1
          fi
          
      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add hh_vacancies.json
          
          if git diff --staged --quiet; then
            echo "‚ö†Ô∏è –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –∫–æ–º–º–∏—Ç–∞"
          else
            VACANCY_COUNT=$(grep -o '"id"' hh_vacancies.json | wc -l || echo "0")
            COMPANY_COUNT=$(grep -o '"company"' hh_vacancies.json | sort -u | wc -l || echo "0")
            git commit -m "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π: $VACANCY_COUNT –≤–∞–∫–∞–Ω—Å–∏–π –æ—Ç $COMPANY_COUNT –∫–æ–º–ø–∞–Ω–∏–π [$(date '+%Y-%m-%d %H:%M')]"
            git push
          fi
          
      - name: Deploy to FTP
        uses: SamKirkland/FTP-Deploy-Action@v4.3.5
        with:
          server: ${{ secrets.FTP_SERVER }}
          username: ${{ secrets.FTP_USERNAME }}
          password: ${{ secrets.FTP_PASSWORD }}
          protocol: ftp
          port: 21
          local-dir: "./"
          server-dir: "www/Vacancy/"
          state-name: ".ftp-deploy-sync-state.json"
          dry-run: false
          log-level: standard
          exclude: |
            **/.git*
            **/.git*/**
            **/node_modules/**
            **/.github/**
            **/README.md
            **/*.py
            **/.ftp-deploy-sync-state.json
