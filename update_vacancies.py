#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤ —Å HeadHunter API
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∫–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ hh.ru
API HeadHunter –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∫–ª—é—á–∞ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import json
import requests
import time
from datetime import datetime
from urllib.parse import quote

class HeadHunterParser:
    def __init__(self):
        self.base_url = "https://api.hh.ru"
        self.headers = {
            'User-Agent': 'VacancyAggregator/1.0 (gradelift.ru)'
        }
        
    def search_vacancies(self, text="–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", area="113", work_format="remote"):
        """
        –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π —á–µ—Ä–µ–∑ API HeadHunter —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ
        
        Args:
            text: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            area: —Ä–µ–≥–∏–æ–Ω (113 = –†–æ—Å—Å–∏—è)
            work_format: —Ñ–æ—Ä–º–∞—Ç —Ä–∞–±–æ—Ç—ã (remote = —É–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞)
        """
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –≤–∞—à–µ–π —Å—Å—ã–ª–∫–µ
        params = {
            'text': text,
            'area': area,  # 113 = –†–æ—Å—Å–∏—è
            'search_field': 'name',  # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –≤–∞–∫–∞–Ω—Å–∏–∏
            'per_page': 100,  # –ú–∞–∫—Å–∏–º—É–º –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            'page': 0,
            'order_by': 'salary_desc',  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            'search_period': 1,  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
            'only_with_salary': 'true',  # –¢–æ–ª—å–∫–æ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π
            'currency': 'RUR'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–¥–∞–ª–µ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if work_format == "remote":
            params['schedule'] = 'remote'
            
        try:
            print(f"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π: '{text}' –≤ —Ä–µ–≥–∏–æ–Ω–µ {area}")
            print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
            
            response = requests.get(
                f"{self.base_url}/vacancies",
                params=params,
                headers=self.headers,
                timeout=15
            )
            
            print(f"üåê URL –∑–∞–ø—Ä–æ—Å–∞: {response.url}")
            
            if response.status_code == 200:
                data = response.json()
                total_found = data.get('found', 0)
                items = data.get('items', [])
                
                print(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {total_found} –≤–∞–∫–∞–Ω—Å–∏–π")
                print(f"üìÑ –ü–æ–ª—É—á–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(items)} –≤–∞–∫–∞–Ω—Å–∏–π")
                
                return {
                    'items': items,
                    'total_found': total_found,
                    'pages': data.get('pages', 1),
                    'per_page': data.get('per_page', 100)
                }
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ API: {response.status_code}")
                print(f"üìù –û—Ç–≤–µ—Ç: {response.text[:500]}...")
                return {'items': [], 'total_found': 0}
                
        except requests.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
            return {'items': [], 'total_found': 0}
    
    def get_multiple_pages(self, text="–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", max_pages=5):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        all_items = []
        
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        first_result = self.search_vacancies(text=text)
        all_items.extend(first_result['items'])
        
        total_pages = min(first_result.get('pages', 1), max_pages)
        total_found = first_result.get('total_found', 0)
        
        print(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}, –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_found}")
        
        # –ó–∞–ø—Ä–æ—Å—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for page in range(1, total_pages):
            print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page + 1}/{total_pages}")
            
            params = {
                'text': text,
                'area': '113',  # –†–æ—Å—Å–∏—è
                'search_field': 'name',
                'per_page': 100,
                'page': page,
                'order_by': 'salary_desc',
                'search_period': 1,
                'only_with_salary': 'true',
                'schedule': 'remote',
                'currency': 'RUR'
            }
            
            try:
                response = requests.get(
                    f"{self.base_url}/vacancies",
                    params=params,
                    headers=self.headers,
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    items = data.get('items', [])
                    all_items.extend(items)
                    print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(items)} –≤–∞–∫–∞–Ω—Å–∏–π")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}: {response.status_code}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}: {e}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(0.5)
        
        return {
            'items': all_items,
            'total_found': total_found,
            'pages_loaded': len(all_items) // 100 + (1 if len(all_items) % 100 else 0)
        }
    
    def format_salary(self, salary_data):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã"""
        if not salary_data:
            return "–ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
            
        salary_from = salary_data.get('from')
        salary_to = salary_data.get('to')
        currency = salary_data.get('currency', 'RUR')
        gross = salary_data.get('gross', True)
        
        currency_symbol = {
            'RUR': '—Ä—É–±.',
            'USD': '$',
            'EUR': '‚Ç¨',
            'KZT': '—Ç–µ–Ω–≥–µ',
            'UZS': '—Å—É–º'
        }.get(currency, currency)
        
        gross_text = "" if gross else " –Ω–∞ —Ä—É–∫–∏"
        
        if salary_from and salary_to:
            return f"–æ—Ç {salary_from:,} –¥–æ {salary_to:,} {currency_symbol}{gross_text}".replace(',', ' ')
        elif salary_from:
            return f"–æ—Ç {salary_from:,} {currency_symbol}{gross_text}".replace(',', ' ')
        elif salary_to:
            return f"–¥–æ {salary_to:,} {currency_symbol}{gross_text}".replace(',', ' ')
        else:
            return "–ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    
    def format_vacancy(self, vacancy):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏"""
        published_date = vacancy.get('published_at', '')
        if published_date:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
            try:
                dt = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                formatted_date = dt.strftime('%Y-%m-%d')
            except:
                formatted_date = published_date[:10]
        else:
            formatted_date = ''
            
        return {
            'id': vacancy.get('id'),
            'title': vacancy.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
            'company': vacancy.get('employer', {}).get('name', '–ö–æ–º–ø–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞'),
            'salary': self.format_salary(vacancy.get('salary')),
            'publishDate': formatted_date,
            'url': vacancy.get('alternate_url', ''),
            'area': vacancy.get('area', {}).get('name', ''),
            'experience': vacancy.get('experience', {}).get('name', ''),
            'employment': vacancy.get('employment', {}).get('name', ''),
            'schedule': vacancy.get('schedule', {}).get('name', ''),
            'premium': vacancy.get('premium', False),
            'has_test': vacancy.get('has_test', False),
            'response_letter_required': vacancy.get('response_letter_required', False)
        }

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–π —Å HeadHunter API...")
    print("üìÖ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∫–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ hh.ru")
    
    parser = HeadHunterParser()
    
    # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –≤—ã—Å–æ–∫–∏–º–∏ –∑–∞—Ä–ø–ª–∞—Ç–∞–º–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤
    search_queries = [
        "–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä",
        "DevOps engineer", 
        "Senior —Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä",
        "–í–µ–¥—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä",
        "–°—Ç–∞—Ä—à–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä"
    ]
    
    all_vacancies = []
    search_stats = {}
    
    for query in search_queries:
        print(f"\n{'='*50}")
        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
        print(f"{'='*50}")
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result = parser.get_multiple_pages(text=query, max_pages=3)
        
        if result['items']:
            formatted_vacancies = [parser.format_vacancy(v) for v in result['items']]
            all_vacancies.extend(formatted_vacancies)
            search_stats[query] = {
                'found': result['total_found'],
                'loaded': len(formatted_vacancies)
            }
            print(f"‚úÖ –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}': –Ω–∞–π–¥–µ–Ω–æ {result['total_found']}, –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(formatted_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
        else:
            search_stats[query] = {'found': 0, 'loaded': 0}
            print(f"‚ö†Ô∏è  –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}': –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
    unique_vacancies = []
    seen_ids = set()
    
    for vacancy in all_vacancies:
        if vacancy['id'] and vacancy['id'] not in seen_ids:
            unique_vacancies.append(vacancy)
            seen_ids.add(vacancy['id'])
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    unique_vacancies.sort(key=lambda x: x['publishDate'], reverse=True)
    
    print(f"\n{'='*50}")
    print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*50}")
    
    for query, stats in search_stats.items():
        print(f"üìã {query}: –Ω–∞–π–¥–µ–Ω–æ {stats['found']}, –∑–∞–≥—Ä—É–∂–µ–Ω–æ {stats['loaded']}")
    
    print(f"\nüìà –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
    print(f"üéØ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
    
    # –ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç
    salaries = []
    for v in unique_vacancies:
        salary_text = v['salary']
        if '–æ—Ç' in salary_text:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞)
                import re
                numbers = re.findall(r'\d+', salary_text.replace(' ', ''))
                if numbers:
                    salaries.append(int(numbers[0]))
            except:
                pass
    
    if salaries:
        avg_salary = sum(salaries) // len(salaries)
        min_salary = min(salaries)
        max_salary = max(salaries)
        print(f"üí∞ –°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞: {avg_salary:,} —Ä—É–±.".replace(',', ' '))
        print(f"üí∞ –î–∏–∞–ø–∞–∑–æ–Ω: {min_salary:,} - {max_salary:,} —Ä—É–±.".replace(',', ' '))
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    result = {
        'source': 'hh.ru',
        'updated': datetime.now().isoformat() + 'Z',
        'search_parameters': {
            'area': '113',  # –†–æ—Å—Å–∏—è
            'search_field': 'name',
            'order_by': 'salary_desc',
            'search_period': 1,
            'only_with_salary': True,
            'schedule': 'remote'
        },
        'search_queries': list(search_stats.keys()),
        'statistics': {
            'total_found': sum(s['found'] for s in search_stats.values()),
            'total_loaded': len(all_vacancies),
            'unique_vacancies': len(unique_vacancies),
            'avg_salary': avg_salary if salaries else 0,
            'salary_range': {'min': min_salary, 'max': max_salary} if salaries else None
        },
        'vacancies': unique_vacancies[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 100 –ª—É—á—à–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    try:
        with open('hh_vacancies.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ –§–∞–π–ª hh_vacancies.json —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω!")
        print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(result['vacancies'])} –≤–∞–∫–∞–Ω—Å–∏–π")
        print(f"üïí –í—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–∞–∫–∞–Ω—Å–∏–π
        if result['vacancies']:
            print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π:")
            for i, v in enumerate(result['vacancies'][:3], 1):
                print(f"{i}. {v['title']} - {v['company']} - {v['salary']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        exit(1)

if __name__ == "__main__":
    main()
