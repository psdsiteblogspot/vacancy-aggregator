#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ —Å HeadHunter
–†–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ –ø—É–±–ª–∏—á–Ω—ã–π API –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∫–ª—é—á–µ
"""

import json
import requests
from datetime import datetime
import time
import sys

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'base_url': 'https://api.hh.ru/vacancies',
    'search_params': {
        'text': '–°–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä',
        'area': '113',  # –†–æ—Å—Å–∏—è (1 - –ú–æ—Å–∫–≤–∞, 2 - –°–ü–±)
        'search_field': 'name',
        'order_by': 'publication_time',
        'period': '1',  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
        'per_page': '100',
        'page': '0',
        'schedule': 'remote',  # –£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞
        # 'only_with_salary': 'true',  # –¢–æ–ª—å–∫–æ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π
        # 'salary': '100000',  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞
    },
    'headers': {
        'User-Agent': 'GitHub Actions Vacancy Parser/1.0'
    },
    'output_file': 'hh_vacancies.json',
    'max_pages': 5  # –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
}

def format_salary(salary):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ"""
    if not salary:
        return '–ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
    
    parts = []
    if salary.get('from'):
        parts.append(f"–æ—Ç {salary['from']:,}".replace(',', ' '))
    if salary.get('to'):
        parts.append(f"–¥–æ {salary['to']:,}".replace(',', ' '))
    
    if parts:
        result = ' '.join(parts)
        currency = salary.get('currency', 'RUR')
        if currency == 'RUR':
            result += ' —Ä—É–±.'
        else:
            result += f' {currency}'
        return result
    
    return '–ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'

def fetch_vacancies(page=0):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    params = CONFIG['search_params'].copy()
    params['page'] = str(page)
    
    try:
        response = requests.get(
            CONFIG['base_url'],
            params=params,
            headers=CONFIG['headers'],
            timeout=30
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
        return None

def parse_vacancy(item):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
    return {
        'title': item['name'],
        'company': item['employer']['name'],
        'salary': format_salary(item.get('salary')),
        'publishDate': item['published_at'].split('T')[0],
        'url': item['alternate_url'],
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        'experience': item.get('experience', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω'),
        'employment': item.get('employment', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
        'schedule': item.get('schedule', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω'),
        'area': item.get('area', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'),
        'description': item.get('snippet', {}).get('requirement', ''),
        'responsibility': item.get('snippet', {}).get('responsibility', ''),
    }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å HeadHunter...")
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞: {CONFIG['search_params']['text']}")
    
    all_vacancies = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    data = fetch_vacancies(0)
    if not data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        sys.exit(1)
    
    total_found = data.get('found', 0)
    pages = data.get('pages', 1)
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_found}")
    print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {pages}")
    
    # –ü–∞—Ä—Å–∏–º –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    for item in data.get('items', []):
        all_vacancies.append(parse_vacancy(item))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    pages_to_load = min(pages, CONFIG['max_pages'])
    for page in range(1, pages_to_load):
        print(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1}/{pages_to_load}...")
        time.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        data = fetch_vacancies(page)
        if data:
            for item in data.get('items', []):
                all_vacancies.append(parse_vacancy(item))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
    all_vacancies.sort(key=lambda x: x['publishDate'], reverse=True)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        'source': 'hh.ru',
        'updated': datetime.now().isoformat() + 'Z',
        'total_found': total_found,
        'vacancies_count': len(all_vacancies),
        'search_params': CONFIG['search_params'],
        'vacancies': all_vacancies
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    try:
        with open(CONFIG['output_file'], 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –≤ {CONFIG['output_file']}")
        print(f"üïí –í—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        print("\nüìå –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –≤–∞–∫–∞–Ω—Å–∏–π:")
        for i, vacancy in enumerate(all_vacancies[:5], 1):
            print(f"\n{i}. {vacancy['title']}")
            print(f"   üíº {vacancy['company']}")
            print(f"   üí∞ {vacancy['salary']}")
            print(f"   üìÖ {vacancy['publishDate']}")
            print(f"   üìç {vacancy['area']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
